{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b307e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\systemx\\anaconda3\\lib\\site-packages (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\systemx\\anaconda3\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82384b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dill as pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47a7738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393eb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_python/final_work_2/train_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee5d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_path = 'data_python/final_work_2/res/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0db1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('data_python/final_work_2/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ae9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transactions_dataset(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
    "                                 save_to_path=None, verbose: bool=False):\n",
    "\n",
    "    preprocessed_frames = []\n",
    "\n",
    "    def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "        res = []\n",
    "        dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                                  if filename.startswith('train')])\n",
    "\n",
    "        start_from = max(0, start_from)\n",
    "        chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "        if verbose:\n",
    "            print('Reading chunks:\\n')\n",
    "            for chunk in chunks:\n",
    "                print(chunk)\n",
    "        for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "            print('chunk_path', chunk_path)\n",
    "            chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "            res.append(chunk)\n",
    "\n",
    "        return pd.concat(res).reset_index(drop=True)\n",
    "    \n",
    "    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                                   desc=\"Transforming transactions data\"):\n",
    "        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,\n",
    "                                                             verbose=verbose)\n",
    "\n",
    "        if save_to_path:\n",
    "            block_as_str = str(step)\n",
    "            if len(block_as_str) == 1:\n",
    "                block_as_str = '00' + block_as_str\n",
    "            else:\n",
    "                block_as_str = '0' + block_as_str\n",
    "            transactions_frame.to_parquet(os.path.join(save_to_path, f'processed_chunk_{block_as_str}.parquet'))\n",
    "\n",
    "        preprocessed_frames.append(transactions_frame)\n",
    "    return pd.concat(preprocessed_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d2a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SystemX\\AppData\\Local\\Temp\\ipykernel_872\\554982723.py:27: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f70c422e964aedaf48d299bdf70e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming transactions data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SystemX\\AppData\\Local\\Temp\\ipykernel_872\\554982723.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48349228abf3493685e0049f8e0d55d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path data_python/final_work_2/train_data/train_data_0.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_1.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_10.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_11.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_2.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_3.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_4.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_5.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_6.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_7.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_8.pq\n",
      "chunk_path data_python/final_work_2/train_data/train_data_9.pq\n"
     ]
    }
   ],
   "source": [
    "df = prepare_transactions_dataset(path, num_parts_to_preprocess_at_once=12, num_parts_total=2,\n",
    "                                  save_to_path=save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4dcd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(df):\n",
    "    for m in ['pre_since_confirmed',\n",
    "          'pre_pterm',\n",
    "          'pre_fterm',\n",
    "          'pre_till_fclose',\n",
    "          'pre_loans_credit_limit']:\n",
    "        data_val = df[m].value_counts()\n",
    "        for a, b in zip(data_val.index, data_val.values):    \n",
    "            if b < data_val.sum()/(len(data_val)*2):\n",
    "                df[m] = df[m].replace(a, a+1)\n",
    "            \n",
    "    for m in df.iloc[:, 30:55].columns:\n",
    "        data_val = df[m].value_counts()\n",
    "        for a, b in zip(data_val.index, data_val.values):    \n",
    "            f = data_val.max()\n",
    "\n",
    "            if data_val.index[0] == 0:\n",
    "                if b == f:\n",
    "                    max_0 = a\n",
    "                else:\n",
    "                    df[m] = df[m].replace(a, 1)\n",
    "            else:\n",
    "                if b == f:\n",
    "                    max_1 = a\n",
    "                else:\n",
    "                    df[m] = df[m].replace(a, 0)\n",
    "\n",
    "        for a, b in zip(data_val.index, data_val.values):\n",
    "            f = data_val.max()\n",
    "            if b == f and a != 0:\n",
    "                df[m] = df[m].replace(a, 1)\n",
    "    \n",
    "    df = df.drop(['rn', 'pre_loans_total_overdue'], axis=1)\n",
    "    \n",
    "    df_encode_1 = df['id']\n",
    "    columns_l = list(df.columns.values[1:6])\n",
    "    df_1 = pd.get_dummies(df[columns_l], columns=columns_l, drop_first = True)\n",
    "    dummy_signs = df_1.columns.values\n",
    "    df_concat = pd.concat([df_encode_1, df_1], axis=1)\n",
    "    df_encode_1 = df_concat.groupby(\"id\")[dummy_signs].sum().reset_index(drop=False)\n",
    "    \n",
    "    df_encode_2 = df['id']\n",
    "    columns_l = list(df.columns.values[6:13])\n",
    "    df_2 = pd.get_dummies(df[columns_l], columns=columns_l, drop_first = True)\n",
    "    dummy_signs = df_2.columns.values\n",
    "    df_concat = pd.concat([df_encode_2, df_2], axis=1)\n",
    "    df_encode_2 = df_concat.groupby(\"id\")[dummy_signs].sum().reset_index(drop=False)\n",
    "    \n",
    "    df_encode_3 = df['id']\n",
    "    columns_l = list(df.columns.values[13:25])\n",
    "    df_3 = pd.get_dummies(df[columns_l], columns=columns_l, drop_first = True)\n",
    "    dummy_signs = df_3.columns.values\n",
    "    df_concat = pd.concat([df_encode_3, df_3], axis=1)\n",
    "    df_encode_3 = df_concat.groupby(\"id\")[dummy_signs].sum().reset_index(drop=False)\n",
    "    \n",
    "    df_encode_4 = df['id']\n",
    "    columns_l = list(df.columns.values[25:59])\n",
    "    df_4 = pd.get_dummies(df[columns_l], columns=columns_l, drop_first = True)\n",
    "    dummy_signs = df_4.columns.values\n",
    "    df_concat = pd.concat([df_encode_4, df_4], axis=1)\n",
    "    df_encode_4 = df_concat.groupby(\"id\")[dummy_signs].sum().reset_index(drop=False)\n",
    "    \n",
    "    df_result = pd.concat([df_encode_1, df_encode_2, df_encode_3, df_encode_4, targets], axis=1).drop(columns='id')\n",
    "    \n",
    "#     print(f'Число столбцов в df - {len(df_result.columns)}')\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7041a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_result):\n",
    "    \n",
    "    X, y = df_result.drop(['flag'], axis=1), df_result['flag']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425bb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = torch.from_numpy(X.to_numpy().astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_1 = nn.Linear(303, int(303*2))\n",
    "        self.f1 = nn.ReLU()\n",
    "        self.hidden_2 = nn.Linear(int(303*2), int(303*2)//4)\n",
    "        self.f2 = nn.ReLU()\n",
    "        self.hidden_3 = nn.Linear(int(303*2)//4, (int(303*2)//4)//4)\n",
    "        self.f3 = nn.ReLU()\n",
    "        self.hidden_4 = nn.Linear((int(303*2)//4)//4, ((int(303*2)//4)//4)//4)        \n",
    "        self.f4 = nn.ReLU()        \n",
    "        self.output = nn.Linear(((int(303*2)//4)//4)//4, 1)\n",
    "        self.f5 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.f1(self.hidden_1(x))\n",
    "        x = self.f2(self.hidden_2(x))\n",
    "        x = self.f3(self.hidden_3(x))\n",
    "        x = self.f4(self.hidden_4(x))\n",
    "        x = self.f5(self.output(x))\n",
    "        return x\n",
    "     \n",
    "    def fit(self, X_train, lr=0.001, num_epochs=100):\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(my_net.parameters(), lr=0.001)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            MyDataset(X_train[0], X_train[2]),\n",
    "            batch_size=128,\n",
    "            shuffle=True) \n",
    "        \n",
    "        for epoch in range(num_epochs):    \n",
    "            for X, y in train_dataloader:\n",
    "                pred = my_net(X)\n",
    "                loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(f'ROC AUC train - {roc_auc_score(X_train[2], my_net(torch.from_numpy(X_train[0].to_numpy().astype(np.float32))).detach().numpy())}')        \n",
    "        return my_net\n",
    "\n",
    "    def predict(self, X_train):\n",
    "        with torch.no_grad():\n",
    "            y_pred = my_net(torch.from_numpy(X_train[1].to_numpy().astype(np.float32)))\n",
    "        print(f'ROC AUC test - {roc_auc_score(X_train[3], y_pred.detach().numpy())}')\n",
    "        return y_pred\n",
    "\n",
    "my_net = MyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88bf8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059f9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = Pipeline([\n",
    "    ('transformation', FunctionTransformer(transformation)),\n",
    "    ('train_test', FunctionTransformer(train_test)),\n",
    "    ('nn_model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6914fd34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC train - 0.7686966092687515\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = final_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a81b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC test - 0.7593837363087456\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = final_pipe.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e72205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0c00f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit_risk_model.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'model': final_pipe,\n",
    "        'metadata': {\n",
    "            'name': \"credit_risk_model\",\n",
    "            'author': 'Denis Shkaraburov',\n",
    "            'version': 1,\n",
    "            'date': datetime.now()\n",
    "        }\n",
    "    }, file, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08f0b7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred\n",
       "0  0.003342\n",
       "1  0.051365\n",
       "2  0.037902\n",
       "3  0.053005\n",
       "4  0.015910"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(y_pred_test.detach().numpy(), columns=[\"y_pred\"])\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c93bacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6bcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
